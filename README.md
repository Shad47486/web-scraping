# web-scraping

# Using BS4, find at least 2 websites that you want to scrap data from - provide that code within a .py file  

# In the code, for each website - create at least one dataframe that has structured data 

# Save each of the dataframes as separate .csv files into a "/data" folder within your repo - be sure to include the .csv files within your repo (make sure they are small, < 25mb) 

Instructions:
# 1st: sudo/pip install needed modules to perform assigment (Pandas, BeautifulSoup)
# 2nd: look for a webiste to scrap (amazon, coindesk, weather, stocks, etc)
# 3rd: Right click anywhere on the website and click on inspect where you will now see an element tab on the side of the broswer
    #3a: you can right click over the data you want to scrap and it will make navigating to it much easier
# 4th: Look for the tag of where the data you want to scrap 
    #4a: if you have muiltple tags you want to scrap make sure they are all the same class and tag
# 5th: go into either one of the python files and it will explain how to use the commands from their and how to save what you wanted to scrap to a datafram/csv file
